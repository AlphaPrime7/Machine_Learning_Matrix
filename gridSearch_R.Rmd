---
title: "R Notebook"
output: html_notebook
---

# GRID SEARCH IN R (DEMONSTRATION PURPOSES ONLY)

-   Grid Searching in R. An exposure to the workflow of Grid Searching and documentation to ensure that this workflow is established.

## Import our dataset

```{r}
ad_data = read.csv("Data/ad.csv",header=FALSE)
head(ad_data, 5)
```

## Data Exploration/Cleansing

```{r}
library(tidyverse)
library(skimr)
ad_data %>% select(-V5) %>% modify_if(is.character, as.factor) %>%
skim() %>% select(-starts_with("numeric.p"))
```

-   A correlation plot will be insane given the number of attributes in this data set so we simply avoid that.
- As a matter of fact, this a supervised reduction of dimensions given how numerous our predictors are. The reduction of dimensions occurs through tree growth (or creation of nodes) that take care of analysis per predictor variable.

## Data Modeling for Decision Tree

```{r}
#CV APPROACH
library(tidymodels)
set.seed(123)
split <- initial_split(ad_data)
train_data <- training(split) #or use my function
test_data <- testing(split)
cv <- vfold_cv(train_data) #cross-validation of the data

#BOOTSTRAP
set.seed(123)
train_data_boot <- bootstraps(train_data, times = 10, apparent = TRUE)
```

## Pre-processing (Recipe)

```{r}
rec <- recipe(V1559 ~ ., train_data) %>% 
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% 
  step_novel(all_nominal()) %>%
  step_impute_median(all_numeric(), -all_outcomes(), -has_role("id vars"))
```

## Machine Learning Model Specification

```{r}
library(parsnip)
library(rpart)
set.seed(100)
model_bag_ads <- bag_tree() %>% #bag is the aggregation method (gradient boosts or random forests)
  set_mode("regression") %>%
  set_engine("rpart", times = 10)
```

- The model is created and the choice here is the bag_tree aggregator.
- There is the random_forest() and boost_tree() methods to choose from.
- Highly sophisticated algorithms we will delve deeper into.

## Model Application to Recipe

```{r}
library(baguette)
bag_ads_app <- workflow() %>% 
  add_recipe(rec) %>%
  add_model(model_bag_ads)
```

- The model is applied to the recipe or pre-process created earlier. The recipe is very important in the success of this step.
- I have not stumbled upon any ideas on R's robustness in handling missing values or problems within data.
- It seems programs like SPSS possess the robustness needed to deal with issues in the data.
- Also , I am unsure of the tree growing algorithm used in R or other programs. A one found in SPSS is CHAD (Chi-square Automatic interaction Detection).

## Fit Model
```{r}
set.seed(100)
library(future)
plan(multisession) #running multiple R sessions 
fit_bag_ads_app <- fit_resamples(bag_ads_app, cv,metrics = metric_set(rmse, rsq),
      control = control_resamples(verbose = TRUE, save_pred = TRUE,
      extract = function(x) extract_model(x)))

dectree_fit_extracts <- function(x){
  for(i in x){
    print(i)
  }
}
dectree_fit_extracts(fit_bag_ads_app)
```

- The model is then fitted to more training data from the CV samples generated above.
- The model can also be fitted to the bootstrap samples as well.

## Root Extraction
```{r}
bag_roots_extract <-  function(x){
  x %>% 
    select(.extracts) %>% 
    unnest(cols = c(.extracts)) %>% 
    mutate(models = map(.extracts,
                        ~.x$model_df)) %>% 
    select(-.extracts) %>% 
    unnest(cols = c(models)) %>% 
    mutate(root = map_chr(model,
                          ~as.character(.x$fit$frame[1, 1]))) %>%
    select(root)  
}
bag_roots_extract(fit_bag_ads)
```

## Visualizing Roots
```{r}
library(forcats)
bag_roots_extract(fit_bag_ads_app) %>% 
  ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + 
  geom_bar() + 
  coord_flip() + 
  labs(x = "root", y = "count") +
  theme(axis.text.y = element_text(size = 9, angle = 30))
```

- The results show the best predictors for the Y variable (ad click).
- This approach is questionable for the regression approach shown above because we have a binary dichotomous Y variable.
- A logistic regression approach should be used and I will dive deeper into the package to determine if a binomial regression method is allowed.
- Hopefully, the {parsnip} package is able to deal with this type of data.

# RANDOM FORESTS

## Model Specification

```{r}
set.seed(100)
model_rf_ads <-rand_forest() %>%
  set_engine("ranger",
             num.threads = parallel::detectCores(), 
             importance = "permutation", 
             verbose = TRUE) %>% 
  set_mode("regression") %>% 
  set_args(trees = 1000)
```

## Model Application to Recipe

```{r}
rf_ads_app <- workflow() %>% 
  add_model(model_rf_ads) %>% 
  add_recipe(rec)
```

